{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow.Keras를 사용한 RNN 모델링."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. RNN Layer 구조:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, LSTM, Bidirectional, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SimpleRNN layer의 은닉층에서 그대로 출력된다. <br>\n",
    "파라미터의 수 = Input_dim * Hidden_dim + Hidden_dim * Hidden_dim + Hidden_dim = 10 * 3 + 3 * 3 + 3 = 30 + 9 + 3 = 42. <br>\n",
    "SimpleRNN layer의 default activation은 tanh() 이다! <br>\n",
    "return_sequences의 default 값은 False 이다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn (SimpleRNN)      (None, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name=\"RNN1\")\n",
    "my_model.add(SimpleRNN(units = 3, input_shape=(2,10)))                # Hidden dimension = 3, sequence length = 2 & input dimension = 10.\n",
    "#my_model.add(SimpleRNN(units = 3, input_length=2, input_dim=10))     # 위와 같은 의미!\n",
    "my_model.summary()                                                    # 파라미터의 수는 sequence length는 무관하다!!! 최종 스텝에서 한 번의 출력이 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch의 입력 크기도 명시할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_1 (SimpleRNN)    (8, 3)                    42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name=\"RNN2\")\n",
    "my_model.add(SimpleRNN(units=3, batch_input_shape=(8,2,10)))   # batch size = 8, sequence length = 2 & input dimension = 10.\n",
    "my_model.summary()                      # 파라미터의 수는 batch size나 sequence length와는 무관하다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return_sequences = True와 같이 설정하면 매 스텝마다 출력이 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_2 (SimpleRNN)    (8, 2, 3)                 42        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name=\"RNN3\")\n",
    "my_model.add(SimpleRNN(units=3, batch_input_shape=(8,2,10), return_sequences=True))  # 매 스텝마다 출력이 있다!\n",
    "my_model.summary()                       # 파라미터의 수는 batch size, sequence length 또는 return sequences 여부와는 무관하다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제는 출력층을 하나 추가해 본다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_3 (SimpleRNN)    (8, 2, 3)                 42        \n",
      "                                                                 \n",
      " dense (Dense)               (8, 2, 5)                 20        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 62\n",
      "Trainable params: 62\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name=\"RNN4\")\n",
    "my_model.add(SimpleRNN(units = 3, batch_input_shape=(8,2,10), return_sequences=True)) \n",
    "my_model.add(Dense(units = 5))                    # Hidden_dim * Output_dim + Output_dim = 3*5 + 5 = 20 개의 파라미터가 추가적으로 필요하다.\n",
    "my_model.summary()                                # 파라미터의 수는 batch size, sequence length 또는 return sequences 여부와는 무관하다!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Embedding Layer의 역할과 구조:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"RNN5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_4 (SimpleRNN)    (None, 30)                3930      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,930\n",
      "Trainable params: 3,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 단순한 RNN.\n",
    "my_model = Sequential(name=\"RNN5\")\n",
    "my_model.add(SimpleRNN(units = 30, input_shape=(20,100)))             # Hidden dimension = 30, sequence length = 20 & input dimension = 100.\n",
    "my_model.summary() # 파라미터의 수 = Input_dim * Hidden_dim + Hidden_dim * Hidden_dim + Hidden_dim = 100 * 30 + 30 * 30 + 30 = 3930"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding + RNN.\n",
    "n_input = 1000                    # 단어의 가지수라 해석할 수 있다. \n",
    "n_emb = 100                       # n_emb < n_input!\n",
    "n_seq = None\n",
    "n_hidden = 30\n",
    "my_model = Sequential(name=\"Embedding_RNN\")\n",
    "my_model.add(Embedding(n_input, n_emb, name=\"Embedding\"))\n",
    "my_model.add(SimpleRNN(units = n_hidden, input_shape=(n_seq,n_emb), name=\"RNN\"))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Embedding_RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " Embedding (Embedding)       (None, None, 100)         100000    \n",
      "                                                                 \n",
      " RNN (SimpleRNN)             (None, 30)                3930      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 103,930\n",
      "Trainable params: 103,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 내부 구조 출력.\n",
    "# Embedding의 파라미터 수 = n_input * n_emb  = 1000 * 100 = 100000.\n",
    "# Embedding층의 역할은 차원 축소이다! \n",
    "# Embedding층에 정수 인덱스 (< n_index)를 입력하면 차원 축소된 밀집 벡터를 학습하게 된다! ==> 자연어!!!\n",
    "# RNN의 파라미터 수 = n_emb * n_hidden + n_hidden * n_hidden + n_hidden = 100 * 30 + 30 * 30 + 30 = 3930.\n",
    "my_model.summary()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Deep RNN 모델:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Deep RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " simple_rnn_5 (SimpleRNN)    (None, 2, 3)              42        \n",
      "                                                                 \n",
      " simple_rnn_6 (SimpleRNN)    (None, 2, 4)              32        \n",
      "                                                                 \n",
      " simple_rnn_7 (SimpleRNN)    (None, 2, 5)              50        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 124\n",
      "Trainable params: 124\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name=\"Deep RNN\")\n",
    "my_model.add(SimpleRNN(units = 3, input_shape=(2,10), return_sequences = True))    # return_sequences = True 와 같이 설정되어 있다. 파라미터의 수 = Input_dim * Hidden_dim + Hidden_dim * Hidden_dim + Hidden_dim = 10*3 + 3*3 + 3 = 42.\n",
    "my_model.add(SimpleRNN(units = 4, return_sequences = True))   # 파라미터의 수 = Hidden_dim1 * Hidden_dim2 + Hidden_dim2 * Hidden_dim2 + Hidden_dim2 =  3*4 + 4*4 + 4 = 32.\n",
    "my_model.add(SimpleRNN(units = 5, return_sequences = True))   # 파라미터의 수 = Hidden_dim2 * Hidden_dim3 + Hidden_dim3 * Hidden_dim3 + Hidden_dim3 =  4*5 + 5*5 + 5 = 50.           \n",
    "my_model.summary()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Bidirectional RNN 모델:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Bi-RNN1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional (Bidirectiona  (None, 6)                84        \n",
      " l)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84\n",
      "Trainable params: 84\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name = \"Bi-RNN1\")\n",
    "my_model.add(Bidirectional(SimpleRNN(3), input_shape=(2,10)))  # 파라미터의 수가 정확하게 두배가 된다.\n",
    "my_model.summary() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Bi-RNN2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_1 (Bidirectio  (None, 6)                84        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 35        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 119\n",
      "Trainable params: 119\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name = \"Bi-RNN2\")\n",
    "my_model.add(Bidirectional(SimpleRNN(3), input_shape=(2,10)))  # 파라미터의 수가 정확하게 두배가 된다.\n",
    "my_model.add(Dense(units = 5))                                 # Output_dim에 해당하는 units만 명시. Bias vector는 단 한개! 3*5 + 3*5 + 5 = 35 개의 파라미터가 추가적으로 필요하다!\n",
    "my_model.summary()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Deep Bi-RNN\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " bidirectional_2 (Bidirectio  (None, 2, 6)             84        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 2, 6)             60        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 2, 6)             60        \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2, 5)              35        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 239\n",
      "Trainable params: 239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "my_model = Sequential(name = \"Deep Bi-RNN\")\n",
    "my_model.add(Bidirectional(SimpleRNN(3,return_sequences=True), input_shape=(2,10))) # 파라미터의 수가 정확하게 두배가 된다.\n",
    "my_model.add(Bidirectional(SimpleRNN(3,return_sequences=True)))                     # 파라미터의 수 = Hidden_dim * Hidden_dim * 6 (with 2 cross terms) + Hidden_dim * 2 (2 biases)\n",
    "my_model.add(Bidirectional(SimpleRNN(3,return_sequences=True)))                     #                  =  3 * 3 * 6 + 3 * 2 = 54 + 6 = 60.\n",
    "my_model.add(Dense(units = 5))                                                      # 파라미터의 수 = 2*(Hidden_dim * Output_dim) + Output_dim. Bias vector는 단 한개! \n",
    "my_model.summary()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Numpy로 RNN 코딩하기:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 차원 정의.\n",
    "timesteps = 10     # Sequence length.\n",
    "input_dim = 4      # 보통은 word vector의 차원.\n",
    "hidden_dim = 8     # 은닉층의 차원 = 출력의 차원!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열 생성.\n",
    "inputs = np.random.random((timesteps, input_dim))     # 입력은 2D 배열.\n",
    "hidden_state_t = np.zeros((hidden_dim,))             # rank 1 배열이며 차원수 = hidden_dim. 0 으로 초기화."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 행렬을 정의한다.\n",
    "Wx = np.random.random((hidden_dim, input_dim))     # (8, 4) : input -> hidden으로 가는 가중치 행렬.\n",
    "Wh = np.random.random((hidden_dim, hidden_dim))   # (8, 8) : hidden -> hidden으로 가는 가중치 행렬.\n",
    "b = np.random.random((hidden_dim,))               # (8,)   : bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 8)\n",
      "(2, 8)\n",
      "(3, 8)\n",
      "(4, 8)\n",
      "(5, 8)\n",
      "(6, 8)\n",
      "(7, 8)\n",
      "(8, 8)\n",
      "(9, 8)\n",
      "(10, 8)\n"
     ]
    }
   ],
   "source": [
    "# Time step을 전개한다.\n",
    "total_hidden_states = []\n",
    "\n",
    "for input_t in inputs:          # 입력 행렬의 개개 행을 가져온다!!!\n",
    "    output_t = np.tanh(np.dot(Wx,input_t) + np.dot(Wh,hidden_state_t) + b)     #  Ot = tanh(Wx . Xt) + tanh(Wh . Ht-1) + b.\n",
    "    total_hidden_states.append(list(output_t))                                 # hidden states를 저장해 간다.\n",
    "    print(np.shape(total_hidden_states))                                       # 계속 커가는 상태를 출력해 준다. \n",
    "    hidden_state_t = output_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.982 0.934 0.958 0.953 0.904 0.873 0.932 0.893]\n",
      " [1.    1.    1.    1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    0.999 1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    1.    1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    0.999 1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    0.998 1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    1.    1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    0.999 1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    1.    1.    1.    1.    1.    1.   ]\n",
      " [1.    1.    1.    1.    1.    1.    1.    1.   ]]\n"
     ]
    }
   ],
   "source": [
    "# sequence 출력.\n",
    "total_hidden_states = np.stack(total_hidden_states, axis = 0)                 # Stack the hidden states (which are also outputs).\n",
    "print(total_hidden_states.round(3))                                           # dim = (timesteps, output_dim)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
